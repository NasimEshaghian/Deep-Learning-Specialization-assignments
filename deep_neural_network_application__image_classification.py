# -*- coding: utf-8 -*-
"""Deep Neural Network Application_ Image Classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YumEsgHA-9GjxNvu6adwEYU8k5Sxyfnc
"""

import numpy as np
import matplotlib.pyplot as plt
import h5py
import scipy
from PIL import Image
from scipy import ndimage

!pip install kaggle

from google.colab import files
files.upload()

!mkdir ~/.kaggle

!mv kaggle.json /root/.kaggle/kaggle.json

!kaggle datasets download -d muhammeddalkran/catvnoncat

!mkdir Dataset

! unzip catvnoncat.zip -d Dataset

def load_dataset():
    train_dataset = h5py.File('Dataset/catvnoncat/train_catvnoncat.h5', "r")
    train_set_x_orig = np.array(train_dataset["train_set_x"][:]) # your train set features
    train_set_y_orig = np.array(train_dataset["train_set_y"][:]) # your train set labels

    test_dataset = h5py.File('Dataset/catvnoncat/test_catvnoncat.h5', "r")
    test_set_x_orig = np.array(test_dataset["test_set_x"][:]) # your test set features
    test_set_y_orig = np.array(test_dataset["test_set_y"][:]) # your test set labels

    classes = np.array(test_dataset["list_classes"][:]) # the list of classes

    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))
    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))

    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes

train_origin , train_origin_y , test_origin , test_origin_y , classes = load_dataset()

train_origin_flatten = train_origin.reshape(train_origin.shape[1]*train_origin.shape[2]*train_origin.shape[3] , train_origin.shape[0])
test_origin_flatten = test_origin.reshape(test_origin.shape[1]*test_origin.shape[2]*test_origin.shape[3] , test_origin.shape[0])

train_origin = train_origin_flatten/255
test_origin = test_origin_flatten/255

n_x = train_origin.shape[0]
n_h = 7
n_y = 1
layer_dims = (n_x, n_h, n_y)
n_x

def sigmoid(z):
  A = 1/(1 + np.exp(-z))
  cache = z
  return A, cache

def relu(z):
  A = np.maximum(0 , z)
  assert(A.shape == z.shape)
  cache = z
  return A, cache

def initialize(n_input, n_hidden, n_output):

  np.random.seed(1)
  W1 = np.random.randn(n_hidden , n_input) * 0.01
  b1 = np.zeros((n_hidden , 1))
  W2 = np.random.randn(n_output , n_hidden) * 0.01
  b2 = np.zeros((n_output , 1))

  params = {"W1" : W1,
            "b1" : b1,
            "W2" : W2,
            "b2" : b2}

  return params

def initialize_deep(layer_dim):
  np.random.seed(3)
  params = {}
  L = len(layer_dims)

  for l in range(1, L):
    params['W' + str(l)] = np.random.randn(layer_dims[l],layer_dims[l-1]) * 0.01
    params['b' + str(l)] = np.zeros((layer_dims[l],1))

  assert(params['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))
  assert(params['b' + str(l)].shape == (layer_dims[l], 1))

  return params

def propagate(A, w, b):
  z = np.dot(w , A) + b
  cache = (A, w, b)
  return z, cache

def activation_forward(prev_A, w, b, activation_function):

  if activation_function == "sigmoid":
    z, linear_cache = propagate(prev_A, w, b)
    A, activation_cache = sigmoid(z)

  elif activation_function == "relu":
    z, linear_cache = propagate(prev_A, w, b)
    A, activation_cache = relu(z)

  cache = (linear_cache, activation_cache)

  return A, cache

def propagate_deep(X, params):

  caches = []
  A = X
  L = len(params) // 2

  for i in range(1, L):
    A_prev = A
    A, cache = activation_forward(A, params["W" + str(i)], params["b" + str(i)], "relu")
    caches.append(cache)

  AL, cache = activation_forward(A, params["W" + str(L)], params["b" + str(L)], "sigmoid")
  caches.append(cache)

  return AL, caches

def compute_cost(AL, Y):

  m = Y.shape[1]
  cost = (-1/m) * (np.dot(Y, np.log(AL).T) + np.dot((1-Y), np.log(1-AL).T))
  cost = np.squeeze(cost)

  return cost

def backpropagation(dz, cache):

  A_prev, w, b = cache
  m = A_prev.shape[1]

  dw = (1/m) * np.dot(dz , A_prev.T)
  db = (1/m) * np.sum(dz, axis = 1, keepdims = True)
  dA_prev = np.dot(w.T , dz)

  return dA_prev, dw, db

def relu_backward(dA, cache):

  Z = cache
  dZ = np.array(dA , copy = True)
  dZ[Z <= 0] = 0

  return dZ

def sigmoid_backward(dA, cache):

  z = cache
  s = 1/(1 + np.exp(-z))
  dZ = dA * s * (1-s)

  return dZ

def activation_backward(dA, cache, activation):
  linear_cache, activation_cache = cache

  if activation == "relu":
    dZ = relu_backward(dA, activation_cache)
  elif activation == "sigmoid":
    dZ = sigmoid_backward(dA, activation_cache)

  dA_prev, dW, db = backpropagation(dZ, linear_cache)
  return dA_prev, dW, db

def deep_backpropagation(AL, Y, caches):
  grads = {}
  L = len(caches) # the number of layers
  m = AL.shape[1]
  Y = Y.reshape(AL.shape)

  dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))

  current_cache = caches[L-1] # Last Layer
  grads["dA" + str(L-1)], grads["dW" + str(L)], grads["db" + str(L)] = activation_backward(dAL, current_cache, "sigmoid")

  for l in reversed(range(L-1)):
    current_cache = caches[l]
    dA_prev_temp, dW_temp, db_temp = activation_backward(grads["dA" + str(l + 1)], current_cache, activation = "relu")
    grads["dA" + str(l)] = dA_prev_temp
    grads["dW" + str(l + 1)] = dW_temp
    grads["db" + str(l + 1)] = db_temp

  return grads

def optimize(parameters, grads, learning_rate):

  L = len(parameters) // 2

  for l in range(L):
    parameters["W" + str(l+1)] = parameters["W" + str(l+1)] - learning_rate * grads["dW" + str(l+1)]
    parameters["b" + str(l+1)] = parameters["b" + str(l+1)] - learning_rate * grads["db" + str(l+1)]

  return parameters

def two_layer_model(X, Y, layer_dims, learning_rate, num_iteration):

  np.random.seed(1)
  n_input, n_hidden, n_output = layer_dims

  params = initialize(n_input, n_hidden, n_output)

  grads = {}

  costs = []

  w1 = params["W1"]
  b1 = params["b1"]
  w2 = params["W2"]
  b2 = params["b2"]

  for i in range(0 , num_iteration):

    A1, cache1 = activation_forward(X, w1, b1, "relu")
    A2, cache2 = activation_forward(A1, w2, b2, "sigmoid")

    cost = compute_cost(A2, Y)

    dA2 = - (np.divide(Y, A2) - np.divide(1 - Y, 1 - A2))
    dA1, dw2, db2 = activation_backward(dA2, cache2, "sigmoid")
    dX, dw1, db1 = activation_backward(dA1, cache1, "relu")

    grads['dW1'] = dw1
    grads['db1'] = db1
    grads['dW2'] = dw2
    grads['db2'] = db2

    parameters = optimize(params, grads, learning_rate)

    w1 = parameters["W1"]
    b1 = parameters["b1"]
    w2 = parameters["W2"]
    b2 = parameters["b2"]

    if i % 100 == 0:
      costs.append(cost)

  return costs, parameters

costs, parameters =two_layer_model(train_origin, train_origin_y, layer_dims = (n_x, n_h, n_y), learning_rate = 0.0075, num_iteration = 2500)
plt.plot(np.squeeze(costs))
plt.xlabel('num of iteration')
plt.ylabel('costs')
plt.title('learninn rate = 0.0075')
plt.show()

def L_layer_model(X, Y, layers_dims, learning_rate, num_iterations):

  np.random.seed(1)
  costs = []
  parameters = initialize_deep(layers_dims)

  for i in range(0, num_iterations):

    AL, caches = propagate_deep(X, parameters)

    cost = compute_cost(AL, Y)

    grads = deep_backpropagation(AL, Y, caches)

    parameters = optimize(parameters, grads, learning_rate)

    costs.append(cost)

  return costs, parameters

costs, parameters = L_layer_model(train_origin, train_origin_y, layers_dims = (n_x, n_h, n_y), learning_rate = 0.001, num_iterations = 3000)
plt.plot(np.squeeze(costs))
plt.xlabel('num of iteration')
plt.ylabel('costs')
plt.title('learning rate = 0.0075')
plt.show()